# -*- coding: utf-8 -*-
"""“CS646_Fall24_HW1.ipynb”的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I3_EVnGP98Ays0fULB3y8PdFcpFH7-1o

#[COMPSCI 646: Information Retrieval - Fall 2024 ](https://people.cs.umass.edu/~rahimi/teaching.html)
#Assignment 1: BM25 retrieval model (Total : 65 points)

**Description**
This is a coding assignment where you will write and execute code to evaluate ranked outputs generated by a retrieval model. Basic proficiency in Python is recommended.  

**Instructions**

* To start working on the assignment, you would first need to save the notebook to your local Google Drive. For this purpose, you can click on *Copy to Drive* button. You can alternatively click the *Share* button located at the top right corner and click on *Copy Link* under *Get Link* to get a link and copy this notebook to your Google Drive.  

*   For questions with descriptive answers, please replace the text in the cell which states "Enter your answer here!" with your answer. If you are using mathematical notation in your answers, please define the variables.

*   For coding questions, you can add code where it says "enter code here" and execute the cell to print the output.



**Submission Details**

* Due date: Oct. 3, 5pm ET
* To create the final PDF submission file, execute *Runtime->RunAll* from the menu to re-execute all the cells and then generate a PDF using *File->Print->Save as PDF*. Make sure that the generated PDF contains all the codes and printed outputs before submission. You are responsible for uploading the correct PDF with all the information required for grading.
* To create the final Python submission file, click on File -> Download -> Download .py.
* Upload the PDF and Python files to [Gradescope](https://www.gradescope.com/courses/).


**Academic Honesty**

Please follow the guidelines under the *Collaboration and Help* section of the slides about course policy.

**Install library**

https://colab.research.google.com/drive/1I3_EVnGP98Ays0fULB3y8PdFcpFH7-1o?usp=sharing
"""

!pip install pyserini
!pip install transformers
!pip install faiss-cpu
!pip install datasets
!pip install pytrec_eval
!pip install bitsandbytes

### Pyserini require updating java version ###

!apt-get install openjdk-21-jdk-headless -qq > /dev/null
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-21-openjdk-amd64"
!update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/jre/bin/java
!java -version

from google.colab import drive
drive.mount('/content/drive')

"""# 1: Building collection index (20 Points)


In this assigment, you will use the [Pyserini](https://github.com/castorini/pyserini/) toolkit to perform BM25 retrieval on the Natural Questions (NQ) dataset of [KILT](https://ai.meta.com/tools/kilt/): [a Benchmark for Knowledge Intensive Language Tasks](https://arxiv.org/pdf/2009.02252).

The first step is to index the document collection of the NQ dataset.
We use the KILT passages set split into 100 words, available [here](https://github.com/facebookresearch/DPR/issues/186). This passage split consists of 35,678,076 passages.

<a name="tsv-file"></a>
First, you need to download the splitted version of the KILT Wikipedia corpus using the code below.
"""

#Download dataset
!wget https://dl.fbaipublicfiles.com/ur/wikipedia_split/psgs_w100.tsv.gz
!gzip -d psgs_w100.tsv.gz

"""**Question 1.1. Preprocessing steps** (10 points)

To index a collection using Pyserini, the first step is to prepare the collection in an appropriate format. A simple format for Pyserini indexing is JSONL files that have two mandatory keys:

```
{"id": "doc1", "contents": "this is the first assignment."}
```

You need to convert the downloaded TSV file into JSONL files.
For this assignment, you only need the four following fields of the TSV files: ```id, text, wikipedia_title, wikipedia_id```.
  
The converted jsonl files for indexing consist of multiple lines in this format:

```
{"id": "1000_0", "contents": "The first passage", "wikipedia_id": "99"}
{"id": "1000_1", "contents": "The second passage", "wikipedia_id": "99"}
...
```
where ```id``` is the id of the passage, ```contents``` is a text that combine ```wikipedia_title``` and ```text``` fields of the TSV file, and ```wikipedia_id``` is the id of the Wikipedia document that contains the passage.

You need to implement the ```convert``` function below to convert the entire wikipedia dataset into JSONL files. We suggest seperating it into several JSONL files (e.g., a maximum of 1 milions lines per file, if the wikipedia tsv file has 3.000.000 lines, then you should convert it into 3 jsonl files, each jsonl file has 1.000.000 lines) to speed up the indexing process later.

Note: If you use the ```pandas``` library to read the TSV file, we suggest using the ```chunksize``` argument to read the TSV file in chunks. This file is relatively large and you may not be able load it completely into RAM.
You can learn more [here](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).
"""

import pandas as pd
import json

# Define the path to your TSV file and the output directory for the JSONL files
tsv_file = '/content/psgs_w100.tsv'  # Path to the downloaded TSV file
output_dir = '/content/drive/MyDrive/HW646/jsonl_files/'  # Output directory for JSONL files

# Set the chunk size to ensure the file is not loaded into memory at once
chunksize = 1000000  # Process 1 million lines at a time

# A set to track processed wikipedia titles to avoid duplicates
processed_titles = set()

# File index to control the output file numbering
file_index = 0

# Function to process a chunk of the TSV data and convert it to JSONL format
def convert_tsv_to_jsonl(tsv_file, output_jsonl_file):
    chunksize = 100000
    with open(output_jsonl_file, 'w') as jsonl_file:
        for chunk in pd.read_csv(tsv_file, sep='\t', chunksize=chunksize):
            for index, row in chunk.iterrows():
                document = {
                    "id": str(row['id']),
                    "contents": f"{row['text']}",
                    "wikipedia_id": str(row['wikipedia_id'])
                }
                jsonl_file.write(json.dumps(document) + '\n')

# Read the TSV file and process each chunk
for chunk in pd.read_csv(tsv_file, sep='\t', chunksize=chunksize):
    process_chunk(chunk, file_index)
    file_index += 1  # Increment the file index after each chunk is processed



"""**Question 1.2. Indexing** (10 points)

Next you need to call the Pyserini indexer function. Note that the index can store raw documents for efficient access in the later steps of the assignment. More information on Pyserini indexer can be found [here](https://github.com/castorini/pyserini/blob/master/docs/usage-index.md). You can run python commands in Google Colab by adding ```!``` before command, e.g ```!python -m pyserini.index.lucene```.

**Important notes**
*   The output index files are relatively large, so be sure to save them on your Google Drive for easy access later.

*   You can use your UMass Google Drive.

*   The default runtime option on Google Colab is CPU. When using the CPU or T4 GPU runtime, you may be limited to setting the number of threads (in the Pyserini indexing command) to 1. As a result,  the indexing process may take a couple of hours.

*   We suggest using TPU runtime for this step if possible.
In our experiments, we used the TPU runtime and set the number of threads to 8; the indexing step was completed in about 20 minutes.
For changing the runtime option, refer to [section below](#change-runtime).

"""

!python -m pyserini.index.lucene \
  --collection JsonCollection \
  --input /content/drive/MyDrive/HW646/jsonl_files/ \
  --index /content/drive/MyDrive/HW646/indexes/lucene-index \
  --generator DefaultLuceneDocumentGenerator \
  --threads 1 \
  --storePositions --storeDocvectors --storeRaw

"""## 2: BM25 Retrieval (15 Points)

In this part, you need to run the BM25 model to retrieve documents (at the passage level) from the Wikipedia dataset that was indexed in the previous part.





To do this, you first need to load the built index and the NQ dataset to access the queries using the code below.


"""

from datasets import load_dataset
from pyserini.search.lucene import LuceneSearcher

index_path = '/content/drive/MyDrive/HW646/indexes/lucene-index/' #change to the path to your own index folder (from question 1)
searcher = LuceneSearcher(index_path)
kilt_nq = load_dataset("kilt_tasks", name="nq", cache_dir='./hf_dataset/')

"""**Question 2.1.** BM25 retrieval results (15 points)



Each example of the ([KILT tasks](https://huggingface.co/datasets/facebook/kilt_tasks)) has three main parts: ```id```, ```input```, and ```output```. The ```output``` part contains several outputs for the ```input```, and each of them has a ```provenance``` field. The provenance field has a list of features (```wikipedia_id```, ```start_paragraph_id```, ```end_paragraph_id```) representing Wikipedia documents and paragraphs that support the answer.

In this part, you need to use ```input``` values as queries.

Then [Pyserini Interactive Searching](https://github.com/castorini/pyserini/blob/master/docs/usage-interactive-search.md) can be used to retrieve documents with respect to these queries.

To speed up searching, you can use ```batch_search``` function:
```
hits = searcher.batch_search(queries, qids=qids, k=top_n, threads=threads)
```
where:
*   queries: list of content of queries
*   qids: list of ids of queries
*   top_n: number of retrieved documents
*   threads: number of threads

You need to implement a function that returns a list of retrieved documents in this format (the numbers in this list are BM25 scores of documents):
```
run = {
    'query_id1': {
        'doc_id1': 0.9,
        'doc_id2': 1.8,
    },
    'query_id2': {
        'doc_id8': 0.8,
        'doc_id9': 1.99,
        'doc_id10': 0.36,
    },
}
```

You can also try different parameter values for the BM25 model to optimize search results over this dataset.




"""

!pip install huggingface_hub

from huggingface_hub import login

login("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")

from datasets import load_dataset
from pyserini.search.lucene import LuceneSearcher
import json
import os

def run_bm25(hf_dataset, searcher, top_n=20):
    run = {}
    queries = []
    qids = []

    for example in hf_dataset:
        qids.append(example['id'])
        queries.append(example['input'])

    print(f"Total queries: {len(queries)}")

    hits = searcher.batch_search(queries, qids=qids, k=top_n, threads=4)

    for qid in hits:
        run[qid] = {}
        for hit in hits[qid]:
            run[qid][hit.docid] = hit.score

    print(f"Queries with results: {len(run)}")
    return run

# Main script for BM25 retrieval
if __name__ == "__main__":
    # Set paths and parameters
    index_path = '/content/drive/MyDrive/HW646/indexes/lucene-index/'
    nq_run_results_path = '/content/drive/MyDrive/HW646/nq_run_result.json'

    # Check if files exist
    print(f"Index path exists: {os.path.exists(index_path)}")

    # Initialize searcher and set BM25 parameters
    searcher = LuceneSearcher(index_path)
    searcher.set_bm25(1.1, 0.3)  # Best parameters from tuning

    # Print index statistics
    print(f"Total documents in index: {searcher.num_docs}")

    # Load dataset
    kilt_nq = load_dataset("kilt_tasks", name="nq", cache_dir='./hf_dataset/')
    print(f"KILT NQ dataset loaded. Validation set size: {len(kilt_nq['validation'])}")

    # Run BM25 retrieval
    nq_run = run_bm25(kilt_nq['validation'], searcher)

    # Save retrieval results
    with open(nq_run_results_path, 'w') as f:
        json.dump(nq_run, f)
    print(f"Run results saved to {nq_run_results_path}")

print("BM25 retrieval completed and results saved.")

"""# 3: Evaluating retrieved results (10 Points)

In this part, you need to measure the quality of the retrieved resutls by the BM25 model in terms of precision and MAP metrics.

You can access the relevance judment file (qrel file) [here](https://drive.google.com/file/d/1x5jBE6XuOp1gUX--nonGW8Vkg8hiYBbG/view?usp=sharing).

This file is created using information from the KILT tasks and the collection file, as shown in the code below. The code is provided for your reference and does not need to be run.

For a query (input field of KILT), all passages in the collection that have a ```wikipedia_id``` available in the corresponding ```provenance``` field of ```input``` are considered relevant.
Note that the ```output``` part of the KILT tasks contains several outputs for the ```input```, and each of them has a ```provenance``` field. The provenance field has a list of features (```wikipedia_id```, ```start_paragraph_id```, ```end_paragraph_id```) representing Wikipedia paragraphs that support the answer. The collection from the [TSV file](#tsv-file) includes ```id, text, wikipedia_title, wikipedia_id``` fields.

The qrel file has the following format:
```
qrel = {
    'query_id_1': {
        'passage_id_1': 1
    },
    'query_id2': {
        'passage_id_10': 1,
        'passage_id_99': 1,
    },
}
```

An example line from the file is:
```
{
      "6915606477668963399": {
            "3894652": 1,
            "3894653": 1,
            "3894654": 1,
            "3894655": 1,
            "3894656": 1,
            "3894657": 1,
            "3894658": 1
      },
...
}
```
which shows passages "3894652", "3894653", "3894654", "3894655", "3894656", "3894657", and "3894658" from the TSV collection are relevant to the query "6915606477668963399".
"""

#This code was used to generate the qrel file

def prepare_judgments(hf_dataset, pandas_dataset):
  """
   Create a relevance judgments list

   :param huggingface_dataset hf_dataset: KILT task dataset loaded from huggingface
   :param dictionary ids_mapping: map wikipedia document ids to paragraph ids
   :return: relevance judgments list of inputs in the hf_dataset
   :rtype: dictionary
  """
  qrels = {}
  ids_mapping = {}
  for index, row in pandas_dataset.iterrows():
    ids_mapping[row['wikipedia_id']] = row['id']
  for entry in hf_dataset:
      query_id = entry['id']
      qrels[query_id] = {}
      for output in entry['output']:
          for provenance in output['provenance']:
              wiki_id = int(provenance['wikipedia_id'])
              for paragraph_id in ids_mapping[wiki_id]:
                qrels[query_id][paragraph_id] = 1
  return qrels

"""You need to load the qrel file using the code below."""

import json
path_to_qrel_file = '/content/drive/MyDrive/HW646/qrels_nq.json' #path to the downloaded qrel file
with open(path_to_qrel_file) as f:
  qrels_nq = json.load(f)

"""**Question 3.1.** Evaluating retrieved results (10 points)

In this part, you need to measure the quality of the retrieved resutls by the BM25 model in terms of precision and MAP metrics, both at top-3, top-5, and top-10.
One tool to be used for this part is [pytrec_eval](https://github.com/cvangysel/pytrec_eval).
You need to return the average scores for all queries in the dev set of the Natural Questions dataset.

"""

import json
import pytrec_eval

def evaluate_results(run, qrels):
    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P_3', 'P_5', 'P_10'})
    results = evaluator.evaluate(run)

    map_score = sum(v['map'] for v in results.values()) / len(results)
    p3_score = sum(v['P_3'] for v in results.values()) / len(results)
    p5_score = sum(v['P_5'] for v in results.values()) / len(results)
    p10_score = sum(v['P_10'] for v in results.values()) / len(results)

    return map_score, p3_score, p5_score, p10_score

# Main script for evaluation
nq_run_results_path = '/content/drive/MyDrive/HW646/nq_run_result.json'
qrels_nq_path = '/content/drive/MyDrive/HW646/qrels_nq.json'

with open(nq_run_results_path, 'r') as f:
    nq_run = json.load(f)

with open(qrels_nq_path, 'r') as f:
    qrels_nq = json.load(f)

map_score, p3_score, p5_score, p10_score = evaluate_results(nq_run, qrels_nq)

print("Evaluation Results:")
print(f"MAP: {map_score:.4f}")
print(f"P@3: {p3_score:.4f}")
print(f"P@5: {p5_score:.4f}")
print(f"P@10: {p10_score:.4f}")

"""# 4: Evaluating the utility of BM25 results for LLMs (20 Points)

In this part, the top-$k$ BM25 results are passed to an LLM to generate answers to the questions in the NQ dataset. This pipeline is referred to as retrieval-augmented generation (RAG).

To generate the answers, we use the 8-bit quantized version of the [Meta Llama 3.1](https://scontent-bos5-1.xx.fbcdn.net/v/t39.2365-6/453304228_1160109801904614_7143520450792086005_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Pn8NYNVUrzUQ7kNvgEHeLLC&_nc_ht=scontent-bos5-1.xx&_nc_gid=AT9Bvt94ViGYeFktr8kjMUn&oh=00_AYCLZFaHl_WqvfasGUihREkjR8rPzSUzCjRMiDIuwv_FXw&oe=66F25007) 8B model.
You can change this to any other open source LLM of your interest, such as [Gemma 2](https://arxiv.org/abs/2408.00118), if you'd like.

After generating the answers to the quesitons, their quality is measured in terms of [Exact Match](https://huggingface.co/spaces/evaluate-metric/exact_match) which assesses whether the generated answers exactly contain the gold answers.

All these components are implemented below and you just need to run them by passing the BM25 results you obtained.

Your task in this part is to study the correlation between the quality of BM25 results and the quality of generated answers to the questions.

**Question 4.1.** Using the inverted index to prepare input for the LLM (6 points)

In this part, you need to create a JSON file for the NQ dataset in this format:

```
dataset =
[{
  "id": "query_1",
  "input": "the first input",
  "answers": ['answer 1', 'answer 2',...]
  "ctxs": [{"id": "paragraph1 id"
            "text": "paragraph1 content"},
            {"id": "paragraph2 id"
            "text": "paragraph2 content"},
            ...
          ]     
},
  {
  "id": "query_2",
  "input": "the second input",
  "answers": ['answer 1', 'answer 2',...]
  "ctxs": [{"id": "paragraph1 id"
            "text": "paragraph1 content"},
            {"id": "paragraph2 id"
            "text": "paragraph2 content"},
            ...
          ]     
},
...
]
```
where "id", "input", and "answers" are obtained from the NQ dataset of KILT, and "ctxs" contains the top-10 retrieved documents from part 2. To obtian the contents of documents for this file, you need to use the built inverted index.
"""

import json
from pyserini.search.lucene import LuceneSearcher
from tqdm import tqdm
from datasets import load_dataset

def get_ctxs(doc_ids, searcher):
    """
    Get the text of given paragraph ids by using the built index.

    :param dictionary doc_ids: paragraph ids and their corresponding BM25 scores
    :param pyserini_lucene searcher: a search object of the built index
    :return: list of paragraphs corresponding to given ids
    :rtype: list
    """
    ctxs = []  # Initialize the list to hold the contexts
    for doc_id in doc_ids:
        # Get the document by its ID from the searcher
        doc = searcher.doc(doc_id)
        if doc:
            ctxs.append({
                'id': doc.docid(),
                'score': doc_ids[doc_id],  # Use the score from BM25 retrieval
                'contents': doc.raw()  # Add the raw contents of the document
            })
    return ctxs

# Path to save the generated JSON
saved_json_path = '/content/drive/MyDrive/HW646/nq.json'

# Don't change the code below
def prepare_json_file(hf_dataset, retrieval_dict, searcher, saved_json_file_path):
    saved_json_list = []
    for entry in tqdm(hf_dataset):
        saved_dict = {'answers': []}
        saved_dict['id'] = entry['id']
        saved_dict['input'] = entry['input']
        for output in entry['output']:
            saved_dict['answers'].append(output['answer'])
        # Get the retrieved paragraphs for the current query
        saved_dict['ctxs'] = get_ctxs(retrieval_dict[entry['id']], searcher)
        saved_json_list.append(saved_dict)

    # Save the result as a JSON file
    with open(saved_json_file_path, 'w') as fout:
        json.dump(saved_json_list, fout, indent=6)

# Assuming `kilt_nq['validation']` is the dataset and `nq_run` contains the BM25 results
prepare_json_file(kilt_nq['validation'], nq_run, searcher, saved_json_path)

"""**Preparation**

<a name="change-runtime"></a>
*GPU*

You need to use GPU for running the code below. In order to connect to GPU, you can click on *Runtime* then select *Change runtime type* to see the options.

*Hugging Face User access tokens*



To exectue the code below you need to obtain an [Access Token](https://huggingface.co/docs/hub/en/security-tokens):

1. Log in to [Hugging Face](https://huggingface.co/login)
2. Navigate to Settings
3. Access Tokens Tab
4. Create a New Token
5. Choose a Name and Scope: Scopes define the permissions granted to the token. “read” access is sufficient for this assignment.
6. Generate and Copy that to ```HF_access_token``` in the code below

*LLAMA 3.1 COMMUNITY LICENSE AGREEMENT*

"You need to agree to share your contact information to access the Llama models". Use this [link](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B) for this step (it will take 10 minutes to several hours to get permission to access the models on Hugging Face).

Then you can load the LLM using the code below.
"""

from huggingface_hub import login
login(token="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")

#Loading model
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

HF_access_token = "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"
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_access_token)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", token=HF_access_token)
config = AutoConfig.from_pretrained(model_name, token=HF_access_token)

"""

**Question 4.2.** (4 points)

In this part, you first get the quality of generated answers by the LLM when retrieved results are used to augment the prompts.

One aspect to study is how varying the number of retrieved results passed to the LLM impacts the quality of its generated answers. For this purpose, you need to report the Exact Match (EM) scores in **four different settings** when


*   no document is passed to the the LLM: the LLM prompt only contains the question.
*   top-3 retrieved documents by BM25 are passed to LLM: the LLM prompt  contains the question and the top-3 retrieved documents.
*   top-5 retrieved documents by BM25 are passed to LLM: the LLM prompt  contains the question and the top-5 retrieved documents.
*   top-10 retrieved documents by BM25 are passed to LLM: the LLM prompt  contains the question and the top-10 retrieved documents.



To get these performance values,  you need to call the
```run_eval``` function below  with the built JSON file including retrieved results and different values for its ```top_n``` variable.










"""

#This code is adopted from https://github.com/AI21Labs/in-context-ralm/
#Some utility functions

import os
import argparse
import json
import re
import string
import torch
from tqdm import tqdm

def normalize_question(question):
  if not question.endswith("?"):
      question = question + "?"

  return question[0].lower() + question[1:]

def normalize_answer(s):
  def remove_articles(text):
      return re.sub(r"\b(a|an|the)\b", " ", text)

  def white_space_fix(text):
      return " ".join(text.split())

  def remove_punc(text):
      exclude = set(string.punctuation)
      return "".join(ch for ch in text if ch not in exclude)

  def lower(text):
      return text.lower()

  return white_space_fix(remove_articles(remove_punc(lower(s))))

def text_has_answer(answers, text) -> bool:
  if isinstance(answers, str):
      answers = [answers]
  text = normalize_answer(text)
  for single_answer in answers:
      single_answer = normalize_answer(single_answer)
      if single_answer in text:
          return True
  return False

def exact_match(prediction, ground_truth):
  return normalize_answer(prediction) == normalize_answer(ground_truth)

def get_answer_from_model_output(outputs, tokenizer, prompt):
  generation_str = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)
  generation_str = generation_str[len(prompt):]
  answer = generation_str.split("\n")[0]
  return answer, generation_str

def load_dataset(dataset_path):
    print("Loading dataset:", dataset_path)
    with open(dataset_path) as f:
        return json.load(f)

#This code is adopted from https://github.com/AI21Labs/in-context-ralm/

def evaluate_dataset(model, tokenizer, device, eval_dataset, max_length, \
                     num_docs=0, output_dir=None, max_tokens_to_generate=10):
    idx = 0
    num_correct = 0
    num_has_answer = 0
    num_too_long = 0
    sample_prompt = None
    for ex in (tq := tqdm(eval_dataset, desc=f"EM:  0.0%")):
        answers = ex["answers"]
        prompt = build_qa_prompt(ex, num_docs=num_docs)
        if idx == 0:
            sample_prompt = prompt
        has_answer = text_has_answer(answers, prompt)
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
        if input_ids.shape[-1] > max_length - max_tokens_to_generate:
            num_too_long += 1
            input_ids = input_ids[..., -(max_length - max_tokens_to_generate):]

        with torch.no_grad():
            outputs = model.generate(input_ids, max_new_tokens=max_tokens_to_generate)

        prediction, generation = get_answer_from_model_output(outputs, tokenizer, prompt)
        is_correct = any([exact_match(prediction, answer) for answer in answers])

        idx += 1
        if is_correct:
            num_correct += 1
        if has_answer:
            num_has_answer += 1
        tq.set_description(f"EM: {num_correct / idx * 100:4.1f}%")

    em = num_correct / idx * 100
    has_answer = num_has_answer / idx * 100
    print(f"EM: {em:.1f}%")
    print(f"% of prompts with answer: {num_has_answer / idx * 100:.1f}%")
    if output_dir is not None:
        d = {"em": em, "has_answer": has_answer, "num_examples": idx, "too_long": num_too_long}
        with open(os.path.join(output_dir, "eval.json"), "w") as f:
            f.write(json.dumps(d) + "\n")
        if sample_prompt is not None:
            with open(os.path.join(output_dir, "example_prompt.txt"), "w") as f:
                f.write(sample_prompt)

def build_qa_prompt(example, num_docs=1):
    if num_docs == 0:
        question_text = normalize_question(example["input"])
        ex_prompt = f"Answer the question:\nQ: {question_text}\nA:"
    elif num_docs == 1:
        q = normalize_question(example["input"])
        text = example['ctxs'][0]['text']
        ex_prompt = f"{text}\n\nBased on this text, answer this question:\nQ: {q}\nA:"
    else:
        q = normalize_question(example["input"])
        docs_text = "\n\n".join([f"{ctx['text']}" for ctx in example["ctxs"][:num_docs]])
        ex_prompt = f"{docs_text}\n\nBased on these texts, answer this question:\nQ: {q}\nA:"
    return ex_prompt

import os
import json
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import matplotlib.pyplot as plt
from collections import Counter
import random
import string
import re

# Use your Hugging Face access token
HF_access_token = "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"

# Load the generator model and tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_access_token)
model = AutoModelForCausalLM.from_pretrained(model_name, token=HF_access_token)

# Set pad_token to eos_token
tokenizer.pad_token = tokenizer.eos_token

# File paths and output directory
json_file = '/content/drive/MyDrive/HW646/nq.json'  # JSON file path generated in 4.1
output_dir = '/content/drive/MyDrive/HW646/'
os.makedirs(output_dir, exist_ok=True)

def normalize_answer(s):
    """Lower text and remove punctuation, articles and extra whitespace."""
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    return white_space_fix(remove_articles(remove_punc(lower(s))))

def exact_match(prediction, ground_truth):
    return normalize_answer(prediction) == normalize_answer(ground_truth)

def calculate_f1_score(prediction, ground_truth):
    pred_tokens = normalize_answer(prediction).split()
    truth_tokens = normalize_answer(ground_truth).split()
    common = Counter(pred_tokens) & Counter(truth_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(pred_tokens)
    recall = 1.0 * num_same / len(truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def extract_answer(text):
    answer = text.split("The answer is")[-1].strip()
    return answer.split(".")[0]  # Take the first sentence

def run_eval(model, tokenizer, json_file, output_dir, top_n=10, max_samples=50):
    with open(json_file, 'r') as f:
        eval_data = json.load(f)

    if len(eval_data) > max_samples:
        eval_data = random.sample(eval_data, max_samples)

    num_correct = 0
    total = 0
    total_f1 = 0.0

    progress_bar = tqdm(eval_data, desc=f"Evaluating top-{top_n}")

    for entry in progress_bar:
        ctx_texts = [ctx.get('text', '') for ctx in entry.get('ctxs', [])[:top_n]]
        contexts = "\n\n".join(ctx_texts)
        prompt = f"{contexts}\n\nQ: {entry['input']}\nA: The answer is"

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024, padding=True)

        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=50,
                pad_token_id=tokenizer.eos_token_id
            )

        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
        prediction = extract_answer(prediction)

        is_correct = any(exact_match(prediction, answer) for answer in entry['answers'])
        best_f1 = max(calculate_f1_score(prediction, answer) for answer in entry['answers'])

        if is_correct:
            num_correct += 1
        total_f1 += best_f1
        total += 1

        em_score = (num_correct / total) * 100
        avg_f1 = (total_f1 / total) * 100
        progress_bar.set_postfix({'EM': f'{em_score:.2f}%', 'F1': f'{avg_f1:.2f}%'})

    final_em_score = (num_correct / total) * 100 if total > 0 else 0
    final_f1_score = (total_f1 / total) * 100 if total > 0 else 0

    results = {
        "em": final_em_score,
        "f1": final_f1_score,
        "num_examples": total,
    }
    with open(os.path.join(output_dir, f"eval_final_top_{top_n}.json"), "w") as f:
        json.dump(results, f, indent=2)

    return final_em_score, final_f1_score

# Evaluation for different numbers of retrieved documents
doc_counts = [0, 3, 5, 10]
results = {}

for top_n in doc_counts:
    print(f"\nEvaluating with top-{top_n} documents:")
    em_score, f1_score = run_eval(model, tokenizer, json_file, output_dir, top_n=top_n)
    results[top_n] = {"EM": em_score, "F1": f1_score}
    print(f"Final EM Score for top-{top_n}: {em_score:.2f}%")
    print(f"Final F1 Score for top-{top_n}: {f1_score:.2f}%")

# Print evaluation results
print("\nSummary of scores:")
for top_n, scores in results.items():
    print(f"Top-{top_n} documents: EM={scores['EM']:.2f}%, F1={scores['F1']:.2f}%")

# Visualize results
plt.figure(figsize=(10, 6))
x = list(results.keys())
em_scores = [scores['EM'] for scores in results.values()]
f1_scores = [scores['F1'] for scores in results.values()]

plt.plot(x, em_scores, marker='o', label='EM Score')
plt.plot(x, f1_scores, marker='s', label='F1 Score')
plt.xlabel('Number of Retrieved Documents')
plt.ylabel('Score (%)')
plt.title('Impact of Retrieved Documents on LLM Performance')
plt.legend()
plt.grid(True)
plt.savefig(f"{output_dir}/performance_plot.png")
plt.close()
print(f"\nA plot of the results has been saved to {output_dir}/performance_plot.png")

"""**Question 4.3.** Analyzing performance results (10 points)

Answer these questions based on the performance results you obtained:

1. Does passing more documents to the LLM lead to higher quality of generated answers?

Yes, passing more documents to the LLM does lead to higher quality of generated answers, but only when a significant number of documents are used. The results show that:

- For 0, 3, and 5 documents, the Exact Match (EM) score remains constant at 2.00%.
- However, when using 10 documents, there's a notable improvement with the EM score increasing to 6.00%.

This suggests that passing more documents can improve answer quality, but the improvement may only be significant when a larger number of documents (in this case, 10) are used.

2.   Does the setting with the highest percision among the four settings above result in the highest exact-match score?

Yes, the setting with the highest precision (F1 score) also results in the highest exact-match score. The top-10 documents setting has both the highest F1 score (6.19%) and the highest EM score (6.00%).

3.  Is there a relationship between the two evaluation metrics, the precision of the retreived results and the exact-match score of the generated answers?

Yes, there appears to be a positive relationship between the precision (F1 score) of the retrieved results and the exact-match score of the generated answers. This relationship is most evident in the top-10 documents setting:

- Top-10 documents have the highest F1 score (6.19%) and also the highest EM score (6.00%).
- For the other settings (0, 3, and 5 documents), both F1 and EM scores are lower and relatively similar to each other.

This suggests that as the precision of the retrieved results improves, the likelihood of generating exact matches also increases. However, it's worth noting that this relationship is not perfectly linear across all settings, as seen in the variations between 0, 3, and 5 documents.

# 5: Extra credits (max: 10 points)

**Question 5.1.** You can change the prompt template in the ```build_prompt``` function above by including document scores from the BM25 model and study how that impacts the quality of the answers generated by the LLM.

**Question 5.2.** You can study how the rank of the relevant document (the one containing the exact-match answer) impacts the quality of the answers generated by the LLM.

**Question 5.3.** You can investigate how constructing LLM prompts with exactly one relevant document and a varying number of noise documents (up to the maximum you can include) impacts the quality of the answers generated by the LLM.

**Question 5.4.** You can propose another aspect regarding how document lists may impact the quality of generated answers and report your findings on that.
"""

#enter your code here
import os
import json
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import matplotlib.pyplot as plt
from collections import Counter
import random
import string
import re
import numpy as np

# Hugging Face access token
HF_access_token = "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"

# Load the generator model and tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_access_token)
model = AutoModelForCausalLM.from_pretrained(model_name, token=HF_access_token)

# Set pad_token to eos_token
tokenizer.pad_token = tokenizer.eos_token

# File paths and output directory
json_file = '/content/drive/MyDrive/HW646/nq.json'
output_dir = '/content/drive/MyDrive/HW646/'
os.makedirs(output_dir, exist_ok=True)

def normalize_answer(s):
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    return white_space_fix(remove_articles(remove_punc(lower(s))))

def exact_match(prediction, ground_truth):
    return normalize_answer(prediction) == normalize_answer(ground_truth)

def calculate_f1_score(prediction, ground_truth):
    pred_tokens = normalize_answer(prediction).split()
    truth_tokens = normalize_answer(ground_truth).split()
    common = Counter(pred_tokens) & Counter(truth_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(pred_tokens)
    recall = 1.0 * num_same / len(truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def extract_answer(text):
    answer = text.split("The answer is")[-1].strip()
    return answer.split(".")[0]  # Take the first sentence

def build_prompt(example, ctx_texts, include_scores=False):
    contexts = ""
    for i, ctx in enumerate(ctx_texts):
        if isinstance(ctx, dict):
            text = ctx.get('text', '')
            score = ctx.get('score', 0)
        else:
            text = ctx
            score = 0
        if include_scores:
            contexts += f"Document {i+1} (Score: {score:.2f}):\n{text}\n\n"
        else:
            contexts += f"Document {i+1}:\n{text}\n\n"
    return f"{contexts}Q: {example['input']}\nA: The answer is"

def analyze_rank_impact(model, tokenizer, json_file, output_dir, max_samples=50):
    with open(json_file, 'r') as f:
        eval_data = json.load(f)

    if len(eval_data) > max_samples:
        eval_data = random.sample(eval_data, max_samples)

    rank_results = {i: {'correct': 0, 'total': 0} for i in range(1, 11)}

    for entry in tqdm(eval_data, desc="Analyzing rank impact"):
        for rank, ctx in enumerate(entry.get('ctxs', [])[:10], start=1):
            if isinstance(ctx, dict):
                ctx_text = ctx.get('text', '')
            else:
                ctx_text = ctx
            prompt = build_prompt(entry, [ctx_text])
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024, padding=True)

            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=50,
                    pad_token_id=tokenizer.eos_token_id
                )

            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
            prediction = extract_answer(prediction)

            is_correct = any(exact_match(prediction, answer) for answer in entry['answers'])

            rank_results[rank]['total'] += 1
            if is_correct:
                rank_results[rank]['correct'] += 1

    for rank, result in rank_results.items():
        if result['total'] > 0:
            result['accuracy'] = (result['correct'] / result['total']) * 100
        else:
            result['accuracy'] = 0

    return rank_results

def run_eval_with_prompt_variation(model, tokenizer, json_file, output_dir, top_n=10, max_samples=50, include_scores=False):
    with open(json_file, 'r') as f:
        eval_data = json.load(f)

    if len(eval_data) > max_samples:
        eval_data = random.sample(eval_data, max_samples)

    num_correct = 0
    total = 0
    total_f1 = 0.0

    progress_bar = tqdm(eval_data, desc=f"Evaluating top-{top_n} with{'out' if not include_scores else ''} scores")

    for entry in progress_bar:
        ctx_texts = [{'text': ctx.get('text', ''), 'score': ctx.get('score', 0)} for ctx in entry.get('ctxs', [])[:top_n]]
        prompt = build_prompt(entry, ctx_texts, include_scores)

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024, padding=True)

        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=50,
                pad_token_id=tokenizer.eos_token_id
            )

        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
        prediction = extract_answer(prediction)

        is_correct = any(exact_match(prediction, answer) for answer in entry['answers'])
        best_f1 = max(calculate_f1_score(prediction, answer) for answer in entry['answers'])

        if is_correct:
            num_correct += 1
        total_f1 += best_f1
        total += 1

        em_score = (num_correct / total) * 100
        avg_f1 = (total_f1 / total) * 100
        progress_bar.set_postfix({'EM': f'{em_score:.2f}%', 'F1': f'{avg_f1:.2f}%'})

    final_em_score = (num_correct / total) * 100 if total > 0 else 0
    final_f1_score = (total_f1 / total) * 100 if total > 0 else 0

    return final_em_score, final_f1_score



def evaluate_with_noise(model, tokenizer, json_file, output_dir, noise_levels=[0, 1, 3, 5, 9], max_samples=50):
    with open(json_file, 'r') as f:
        eval_data = json.load(f)

    if len(eval_data) > max_samples:
        eval_data = random.sample(eval_data, max_samples)

    noise_results = {}

    for noise in noise_levels:
        num_correct = 0
        total = 0
        total_f1 = 0.0

        progress_bar = tqdm(eval_data, desc=f"Evaluating with {noise} noise documents")

        for entry in progress_bar:
            relevant_doc = entry.get('ctxs', [])[:1]
            noise_docs = random.sample(entry.get('ctxs', [])[1:], min(noise, len(entry.get('ctxs', [])) - 1))
            ctx_texts = relevant_doc + noise_docs
            random.shuffle(ctx_texts)

            prompt = build_prompt(entry, ctx_texts)
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024, padding=True)

            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=50,
                    pad_token_id=tokenizer.eos_token_id
                )

            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
            prediction = extract_answer(prediction)

            is_correct = any(exact_match(prediction, answer) for answer in entry['answers'])
            best_f1 = max(calculate_f1_score(prediction, answer) for answer in entry['answers'])

            if is_correct:
                num_correct += 1
            total_f1 += best_f1
            total += 1

            em_score = (num_correct / total) * 100
            avg_f1 = (total_f1 / total) * 100
            progress_bar.set_postfix({'EM': f'{em_score:.2f}%', 'F1': f'{avg_f1:.2f}%'})

        final_em_score = (num_correct / total) * 100 if total > 0 else 0
        final_f1_score = (total_f1 / total) * 100 if total > 0 else 0
        noise_results[noise] = {'EM': final_em_score, 'F1': final_f1_score}

    return noise_results
def analyze_document_diversity(model, tokenizer, json_file, output_dir, max_samples=50):
    with open(json_file, 'r') as f:
        eval_data = json.load(f)

    if len(eval_data) > max_samples:
        eval_data = random.sample(eval_data, max_samples)

    diversity_results = {}

    for top_n in [3, 5, 10]:
        num_correct = 0
        total = 0
        total_f1 = 0.0
        total_diversity = 0.0

        progress_bar = tqdm(eval_data, desc=f"Evaluating diversity with top-{top_n}")

        for entry in progress_bar:
            ctx_texts = entry.get('ctxs', [])[:top_n]

            # Calculate diversity score (e.g., based on unique words)
            unique_words = set()
            total_words = 0
            for ctx in ctx_texts:
                if isinstance(ctx, dict):
                    text = ctx.get('text', '')
                else:
                    text = ctx
                words = text.split()
                unique_words.update(words)
                total_words += len(words)

            diversity_score = len(unique_words) / total_words if total_words > 0 else 0
            total_diversity += diversity_score

            prompt = build_prompt(entry, ctx_texts)
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024, padding=True)

            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=50,
                    pad_token_id=tokenizer.eos_token_id
                )

            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
            prediction = extract_answer(prediction)

            is_correct = any(exact_match(prediction, answer) for answer in entry['answers'])
            best_f1 = max(calculate_f1_score(prediction, answer) for answer in entry['answers'])

            if is_correct:
                num_correct += 1
            total_f1 += best_f1
            total += 1

            em_score = (num_correct / total) * 100
            avg_f1 = (total_f1 / total) * 100
            progress_bar.set_postfix({'EM': f'{em_score:.2f}%', 'F1': f'{avg_f1:.2f}%', 'Diversity': f'{diversity_score:.2f}'})

        final_em_score = (num_correct / total) * 100 if total > 0 else 0
        final_f1_score = (total_f1 / total) * 100 if total > 0 else 0
        avg_diversity = total_diversity / total if total > 0 else 0
        diversity_results[top_n] = {'EM': final_em_score, 'F1': final_f1_score, 'Avg_Diversity': avg_diversity}

    return diversity_results
# Main execution
if __name__ == "__main__":
    # Original evaluation
    print("Original Evaluation:")
    original_em, original_f1 = run_eval_with_prompt_variation(model, tokenizer, json_file, output_dir)
    print(f"Original EM: {original_em:.2f}%, F1: {original_f1:.2f}%")

    # Question 5.1: Include document scores
    print("\nEvaluating with document scores included in prompt:")
    em_with_scores, f1_with_scores = run_eval_with_prompt_variation(model, tokenizer, json_file, output_dir, include_scores=True)
    print(f"EM with scores: {em_with_scores:.2f}%, F1 with scores: {f1_with_scores:.2f}%")

    # Question 5.2: Analyze rank impact
    print("\nAnalyzing impact of relevant document rank:")
    rank_impact_results = analyze_rank_impact(model, tokenizer, json_file, output_dir)
    for rank, result in rank_impact_results.items():
        print(f"Rank {rank}: Accuracy {result['accuracy']:.2f}%")

    # Question 5.3: Evaluate with noise documents
    print("\nEvaluating with varying levels of noise documents:")
    noise_results = evaluate_with_noise(model, tokenizer, json_file, output_dir)
    for noise, scores in noise_results.items():
        print(f"Noise level {noise}: EM {scores['EM']:.2f}%, F1 {scores['F1']:.2f}%")

    # Question 5.4: Document diversity study
    print("\nAnalyzing impact of document diversity on answer quality:")
    diversity_results = analyze_document_diversity(model, tokenizer, json_file, output_dir)
    for top_n, results in diversity_results.items():
        print(f"Top-{top_n}: EM {results['EM']:.2f}%, F1 {results['F1']:.2f}%, Avg Diversity {results['Avg_Diversity']:.2f}")

    # Visualizations
    plt.figure(figsize=(20, 15))

    plt.subplot(2, 2, 1)
    plt.bar(['Original', 'With Scores'], [original_em, em_with_scores])
    plt.title('Impact of Including Scores')
    plt.ylabel('EM Score')

    plt.subplot(2, 2, 2)
    ranks = list(rank_impact_results.keys())
    accuracies = [result['accuracy'] for result in rank_impact_results.values()]
    plt.plot(ranks, accuracies, marker='o')
    plt.title('Impact of Document Rank')
    plt.xlabel('Rank')
    plt.ylabel('Accuracy')

    plt.subplot(2, 2, 3)
    noise_levels = list(noise_results.keys())
    noise_em_scores = [results['EM'] for results in noise_results.values()]
    noise_f1_scores = [results['F1'] for results in noise_results.values()]
    plt.plot(noise_levels, noise_em_scores, marker='o', label='EM')
    plt.plot(noise_levels, noise_f1_scores, marker='s', label='F1')
    plt.title('Impact of Noise Documents')
    plt.xlabel('Number of Noise Documents')
    plt.ylabel('Score')
    plt.legend()

    plt.subplot(2, 2, 4)
    diversity_top_n = list(diversity_results.keys())
    diversity_em_scores = [results['EM'] for results in diversity_results.values()]
    diversity_f1_scores = [results['F1'] for results in diversity_results.values()]
    diversity_scores = [results['Avg_Diversity'] for results in diversity_results.values()]
    plt.plot(diversity_top_n, diversity_em_scores, marker='o', label='EM')
    plt.plot(diversity_top_n, diversity_f1_scores, marker='s', label='F1')
    plt.plot(diversity_top_n, diversity_scores, marker='^', label='Diversity')
    plt.title('Impact of Document Diversity')
    plt.xlabel('Top N Documents')
    plt.ylabel('Score')
    plt.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'extra_credit_results.png'))
    plt.close()

    print(f"\nVisualization saved as 'extra_credit_results.png' in {output_dir}")